# Infortic Scraper Architecture

A scalable web scraping framework designed for easy extension and maintainability.

## ğŸ“ Project Structure

```
infortic/
â”œâ”€ scraper/
â”‚   â”œâ”€ core/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â”œâ”€ base_scraper.py      # Abstract base class for all scrapers
â”‚   â”‚   â”œâ”€ logger.py            # Singleton logger for centralized logging
â”‚   â”‚   â””â”€ db.py                # Database client interface
â”‚   â”œâ”€ lomba/
â”‚   â”‚   â”œâ”€ __init__.py
â”‚   â”‚   â””â”€ infolomba_scraper.py # InfoLomba scraper implementation
â”‚   â””â”€ __init__.py
â”œâ”€ .env                         # Environment variables
â”œâ”€ requirements.txt             # Python dependencies
â”œâ”€ run.py                       # Main runner script
â””â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone or download the project
cd infortic

# Install dependencies
pip install -r requirements.txt
```

### Automated Daily Execution

This project includes **GitHub Actions** for automated daily scraping:

- **Schedule**: Runs every day at midnight Indonesia time (WIB/UTC+7)
- **Command**: Automatically executes `python run.py --run-with-cleaning`
- **Auto-commit**: Commits and pushes results if there are changes
- **Manual Trigger**: Can be triggered manually from GitHub Actions tab

**Setup Steps for GitHub Repository:**
1. Create a new GitHub repository
2. Push this code to the repository
3. The workflow will automatically be available in the Actions tab
4. No additional configuration needed - it uses the built-in `GITHUB_TOKEN`

### 2. Configuration

Edit the `.env` file with your database credentials:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=infortic
DB_USER=postgres
DB_PASSWORD=your_password_here
```

### 3. Running Scrapers

```bash
# Run all scrapers
python run.py all

# Run specific scraper
python run.py infolomba

# Show help
python run.py help
```

## ğŸ—ï¸ Architecture Overview

### Core Interfaces

#### 1. BaseScraper (Abstract Base Class)

All scrapers inherit from `BaseScraper` which provides:

- **Context Manager Support**: Automatic setup and cleanup of browser drivers/HTTP sessions
- **get_page()**: Navigate to a page with optional element waiting
- **fetch_static_page()**: Fetch static HTML content via HTTP
- **save_data()**: Save scraped data to database
- **scrape()**: Abstract method that must be implemented

```python
from scraper.core.base_scraper import BaseScraper

class MyScraper(BaseScraper):
    def scrape(self):
        with self as scraper:
            driver = scraper.get_page("https://example.com")
            # Extract data logic here
            return {"data": extracted_data}
```

#### 2. Logger (Singleton)

Centralized logging with:
- Automatic log rotation (10MB files, 5 backups)
- Console and file output
- Structured log format
- Scraper-specific logging methods

```python
from scraper.core.logger import Logger

logger = Logger()
logger.info("Starting scraper...")
logger.log_scraper_start("MyScraper", "https://example.com")
```

#### 3. DBClient

Database operations with:
- PostgreSQL support
- Environment-based configuration
- Bulk insert operations
- Connection management

```python
from scraper.core.db import DBClient

db = DBClient()
db.insert_many(data_list, "table_name")
```

## ğŸ“ Adding New Scrapers

To add a new scraper, simply:

1. **Create a new scraper file** in the appropriate module directory
2. **Inherit from BaseScraper**
3. **Implement the scrape() method**

Example:

```python
# scraper/news/news_scraper.py
from ..core.base_scraper import BaseScraper

class NewsScraper(BaseScraper):
    def scrape(self):
        with self as scraper:
            driver = scraper.get_page("https://news-site.com")
            # Your scraping logic here
            articles = self._extract_articles(driver)
            self.save_data(articles, "articles")
            return {"articles": articles}
    
    def _extract_articles(self, driver):
        # Implementation details
        pass
```

4. **Register in run.py** (optional):

```python
# Add to run_all_scrapers() function
scrapers = [
    ("InfoLomba", run_infolomba_scraper),
    ("News", run_news_scraper),  # Add your scraper here
]
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DB_HOST` | Database host | localhost |
| `DB_PORT` | Database port | 5432 |
| `DB_NAME` | Database name | infortic |
| `DB_USER` | Database user | postgres |
| `DB_PASSWORD` | Database password | (required) |
| `SCRAPER_TIMEOUT` | Default timeout for operations | 30 |
| `SCRAPER_HEADLESS` | Run browser in headless mode | true |
| `LOG_LEVEL` | Logging level | INFO |

### Dependencies

Key dependencies include:
- **selenium**: Browser automation
- **aiohttp**: Async HTTP requests
- **psycopg2-binary**: PostgreSQL adapter
- **beautifulsoup4**: HTML parsing
- **python-dotenv**: Environment variable management

## ğŸ“Š Features

- âœ… **Context Manager Support**: Automatic resource management
- âœ… **Singleton Logger**: Centralized logging across all scrapers
- âœ… **Database Abstraction**: Easy data persistence
- âœ… **Async Support**: Both sync and async operation modes
- âœ… **Error Handling**: Comprehensive error logging and recovery
- âœ… **Rate Limiting**: Built-in delays to respect website policies
- âœ… **Headless Operation**: Configurable browser mode
- âœ… **Extensible Design**: Easy to add new scrapers

## ğŸ” Example Usage

```python
# Simple usage
from scraper.lomba.infolomba_scraper import InfoLombaScraper

scraper = InfoLombaScraper()
results = scraper.scrape()
print(f"Scraped {len(results['competitions'])} competitions")
```

## Data Cleaning Behavior

The cleaning routine for the `lomba` table in the database ensures that stale data is not processed, maintaining the integrity of new data inserted. Key improvements include:

- **Validated Deletion**: Ensures all records are removed safely using checks for non-null IDs.
- **Comprehensive Logging**: Captures detailed information about the cleaning process for auditability.
- **Robust Error Handling**: Halts the scraping process if cleaning errors occur, preventing further operations with stale data.
- **Improved Exception Management**: Provides clear messaging and prevents data insertion if cleaning fails.
- **Optional Cleaning Control**: The method `insert_lomba_rows()` can skip cleaning if needed by passing `clean_first=False`.

## ğŸ› Troubleshooting

### Common Issues

1. **Database Connection Failed**
   - Check your `.env` file configuration
   - Ensure PostgreSQL is running
   - Verify network connectivity

2. **ChromeDriver Issues**
   - Install Chrome browser
   - Update ChromeDriver via webdriver-manager
   - Check PATH environment variable

3. **Import Errors**
   - Ensure all dependencies are installed: `pip install -r requirements.txt`
   - Check Python path configuration

### Logs

Logs are automatically stored in the `logs/` directory with rotation. Check `logs/scraper.log` for detailed information about scraper operations.

## ğŸ¤ Contributing

1. Follow the established patterns in `BaseScraper`
2. Add appropriate logging using the `Logger` singleton
3. Include error handling and cleanup
4. Document your scraper's specific requirements
5. Test thoroughly before committing

## ğŸ“„ License

This project is part of the Infortic scraping system.
